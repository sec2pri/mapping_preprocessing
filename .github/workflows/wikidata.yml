# Workflow for downloading and saving Wikidata secondary2primary mappings
name: Check and test Wikidata updates

on:
  workflow_dispatch:
  pull_request: # tests whether it is working on PR
   paths:
   - '.github/workflows/wikidata.yml'
  schedule:
    - cron: "0 0 1,15 * *"  # Run the workflow on the 1st and 15th day of each month
permissions:
  contents: write
  pages: write
  id-token: write
  issues: write
  
jobs:
  check_new_data:
    runs-on: ubuntu-latest
    name: Check the date of the latest data
    outputs:
      DATE_NEW: ${{ steps.check_download.outputs.DATE_NEW }}
      DATE_OLD: ${{ steps.check_download.outputs.DATE_OLD }}
      NEW_RELEASE: ${{ steps.check_download.outputs.NEW_RELEASE }}
    steps:
      # step 1: check the release date for the latest hmdb files
      - name: Checkout
        uses: actions/checkout@v4
      - name: Check for new hmdb files
        id: check_download
        run: |
          ##Register the date from the previous Wikidata download config file 
          date_old=$date
        
          ##Make directory if not existing already
          mkdir datasources/wikidata/results
          ##Define variable to be used in storing and updating output data (to avoid hardcoding for each change) (tba)
          
          ## Download outdated IDs for chemicals qLever Style
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/chemicalAllRedirects.rq -G https://qlever.cs.uni-freiburg.de/wikidata -o datasources/wikidata/results/metabolites_secID2priID_qlever.tsv
                            
          ## Download outdated IDs for genes and proteins qLever Style
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/geneproteinHumanAllRedirects.rq -G https://qlever.cs.uni-freiburg.de/wikidata -o datasources/wikidata/results/geneProtein_secID2priID_qlever.tsv
         
          ##Check new data, fail job if query timeout has occured
          cd datasources/wikidata/results
          fail_file=''
          for File in *.tsv ##Only for tsv files
          do
            if grep -q TimeoutException "$File"; then
              echo "Query Timeout occurred for file: " "$File" 
              echo "Wikidata data will not be updated"
              head -n 20 "$File" 
              echo "DOWNLOAD_FILE=true" >>$GITHUB_ENV
              fail_file="${fail_file} $File"
            else
              echo "No Query Timeout detected for file: " "$File" 
            fi
          done
          # Store value of fail_file in GITHUB_ENV for the issue
          echo "FAILED=${fail_file}" >> $GITHUB_ENV
          ##Remove previous output files (if existing)
          ##find  . -name 'wikidata*' -exec rm {} \;
          ## Set prefix to Wikidata for renaming new data files
          prefix=$(basename "Wikidata") 
          for f in *.tsv ##Only for tsv files
          do
            ##Find all new data files | Remove the IRIs (prefix) | remove the IRIs (suffix) | remove language annotation | save the file with new name
            cat "$f" | sed 's/<http:\/\/www.wikidata.org\/entity\///g' | sed 's/[>]//g' | sed 's/@en//g' > "${prefix}_$f"
            rm "$f"
          done
          ##Change back to main directory
          cd ../..
          ##Move and overwrite all files from results folder to data folder, to update previous data
          mv -f wikidata/results/* wikidata/data/

