# Workflow for downloading and saving Wikidata secondary2primary mappings
name: Check and test Wikidata updates

on:
  workflow_dispatch:
  pull_request: # tests whether it is working on PR
   paths:
   - '.github/workflows/wikidata.yml'
  schedule:
    - cron: "0 0 1,15 * *"  # Run the workflow on the 1st and 15th day of each month
permissions:
  contents: write
  pages: write
  id-token: write
  issues: write
  
jobs:
  check_new_data:
    runs-on: ubuntu-latest
    name: Check the date of the latest data
    outputs:
      DATE_NEW: ${{ steps.check_download.outputs.DATE_NEW }}
      DATE_OLD: ${{ steps.check_download.outputs.DATE_OLD }}
      NEW_RELEASE: ${{ steps.check_download.outputs.NEW_RELEASE }}
    steps:
      # step 1: check the release date for the latest Wikidata files
      - name: Checkout
        uses: actions/checkout@v4
      - name: Check for new Wikidata files
        id: check_download
        run: |
          ##Register the date from the previous Wikidata download config file  ##Fake date for now, since the data has not been uploaded yet.
          date_old=$date #-d "yesterday" +"%Y-%m-%d"
          ##Execute query for metabdata (TBA)
          
          ##Fake date for now, since the data has not been uploaded yet.
          date_new=$date #-d "today" +"%Y-%m-%d"

          #Store dates to output
          echo "DATE_OLD=$date_old" >> $GITHUB_OUTPUT
          echo "DATE_NEW=$date_new" >> $GITHUB_OUTPUT
          ##Compare the dates and set variable if date_new is more recent
          timestamp1=$(date -d "$date_new" +%s)
          timestamp2=$(date -d "$date_old" +%s)
          if [ "$timestamp1" -gt "$timestamp2" ]; then
            echo 'New release available', "$release"
            echo "NEW_RELEASE=true" >> $GITHUB_OUTPUT
          else
            echo 'No new release available'
          fi
          echo "Date of latest release: $date_new"
          echo "Date of release of the current version: $date_old"

          
  test_new_data_processing:
    name: Processing new data and check updates
    needs: check_new_data
    env:
      DATE_OLD: ${{ needs.check_new_data.outputs.DATE_OLD }}
      DATE_NEW: ${{ needs.check_new_data.outputs.DATE_NEW }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      # step 2: download the recent data 
      - name: Download the recent data
        run: |
          sudo apt-get install xml-twig-tools
          ##Store outputs from previous job in environment variables
          #echo "$DATE_NEW=$DATE_NEW" >> $GITHUB_ENV
          ##Make directory if not existing already
          mkdir datasources/wikidata/results
          
          echo 'Accessing the Wikidata data'          
          ## Download outdated IDs for chemicals qLever Style
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/chemicalAllRedirects.rq -G https://qlever.cs.uni-freiburg.de/wikidata -o datasources/wikidata/results/metabolites_secID2priID_qlever.tsv
          ## Download outdated IDs for genes and proteins qLever Style
          curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/geneproteinHumanAllRedirects.rq -G https://qlever.cs.uni-freiburg.de/wikidata -o datasources/wikidata/results/geneProtein_secID2priID_qlever.tsv
         
          ##Check new data, fail job if query timeout has occured
          cd datasources/wikidata/results
          fail_file=''
          for File in *.tsv ##Only for tsv files
          do
            if grep -q TimeoutException "$File"; then
              echo "Query Timeout occurred for file: " "$File" 
              echo "Wikidata data will not be updated"
              head -n 20 "$File" 
              #echo "DOWNLOAD_FILE=true" >>$GITHUB_ENV
              fail_file="${fail_file} $File"
            else
              echo "No Query Timeout detected for file: " "$File" 
            fi
          done

      # step 3: run bash for Wikidata preprocessing 
      - name: Download the recent data
        run: |
          ## Set prefix to Wikidata for renaming new data files
          prefix=$(basename "Wikidata") 

          ## Data processing
          for f in *.tsv ##Only for tsv files
          do
            ##Find all new data files | Remove the IRIs (prefix) | remove the IRIs (suffix) | remove language annotation | save the file with new name
            cat "$f" | sed 's/<http:\/\/www.wikidata.org\/entity\///g' | sed 's/[>]//g' | sed 's/@en//g' > "${prefix}_$f"
            rm "$f"
          done   

