name: Check and Test Secondary to Primary Mappings

on:
  workflow_dispatch:
    inputs:
      datasource:
        description: 'Data source to check (leave empty for all)'
        required: false
        type: choice
        options:
          - 'all'
          - 'chebi'
          - 'hgnc'
          - 'hmdb'
          - 'ncbi'
          - 'uniprot'
          - 'wikidata'
        default: 'all'
  pull_request:
    paths:
      - '.github/workflows/check_process_data.yml'
  schedule:
    - cron: "0 0 1,15 * *"  # Run the workflow on the 1st and 15th day of each month

permissions:
  contents: write
  pages: write
  id-token: write
  issues: write

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Set up datasource matrix
        id: set-matrix
        run: |
          if [[ "${{ github.event.inputs.datasource }}" == "all" || "${{ github.event.inputs.datasource }}" == "" ]]; then
            echo "matrix={\"datasource\":[\"chebi\",\"hgnc\",\"hmdb\",\"ncbi\",\"uniprot\",\"wikidata\"]}" >> $GITHUB_OUTPUT
          else
            echo "matrix={\"datasource\":[\"${{ github.event.inputs.datasource }}\"]}" >> $GITHUB_OUTPUT
          fi

  check_new_release:
    needs: setup
    runs-on: ubuntu-latest
    name: Check latest release for ${{ matrix.datasource }}
    strategy:
      fail-fast: false
      matrix: ${{fromJson(needs.setup.outputs.matrix)}}
    outputs:
      RELEASE_NUMBER: ${{ steps.check_download.outputs.RELEASE_NUMBER }}
      CURRENT_RELEASE_NUMBER: ${{ steps.check_download.outputs.CURRENT_RELEASE_NUMBER }}
      NEW_RELEASE: ${{ steps.check_download.outputs.NEW_RELEASE }}
      DATE_NEW: ${{ steps.check_download.outputs.DATE_NEW }}
      DATE_OLD: ${{ steps.check_download.outputs.DATE_OLD }}
      COMPLETE_NEW: ${{ steps.check_download.outputs.COMPLETE_NEW }}
      WITHDRAWN_NEW: ${{ steps.check_download.outputs.WITHDRAWN_NEW }}
      UNIPROT_SPROT_NEW: ${{ steps.check_download.outputs.UNIPROT_SPROT_NEW }}
      SEC_AC_NEW: ${{ steps.check_download.outputs.SEC_AC_NEW }}
      DELAC_SP_NEW: ${{ steps.check_download.outputs.DELAC_SP_NEW }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      # Setup Node.js for HGNC
      - name: Setup Node.js
        if: matrix.datasource == 'hgnc'
        uses: actions/setup-node@v3
        with:
          node-version: '20'
      
      # Install Puppeteer for HGNC
      - name: Install Puppeteer
        if: matrix.datasource == 'hgnc'
        run: npm install puppeteer
      
      # Install XML-twig-tools for HMDB
      - name: Install XML tools
        if: matrix.datasource == 'hmdb'
        run: sudo apt-get install -y xml-twig-tools
      
      - name: Check for new ${{ matrix.datasource }} release
        id: check_download
        env:
          DATASOURCE: ${{ matrix.datasource }}
        run: |
          # Read config to get old date
          . datasources/${DATASOURCE}/config
          
          # Set common output variables
          echo "DATE_OLD=$date" >> $GITHUB_OUTPUT
          
          # Source-specific checks
          case "${DATASOURCE}" in
            
            chebi)
              echo 'Accessing the ChEBI archive'
              wget https://ftp.ebi.ac.uk/pub/databases/chebi/archive/ -O chebi_index.html
              echo "CURRENT_RELEASE_NUMBER=$release" >> $GITHUB_OUTPUT
              
              # Extract date from latest release
              date_new=$(tail -4 chebi_index.html | head -1 | grep -oP '<td align="right">\K[0-9-]+\s[0-9:]+(?=\s+</td>)' | awk '{print $1}')
              release=$(tail -4 chebi_index.html | head -1 | grep -oP '(?<=a href="rel)\d\d\d')
              
              echo "RELEASE_NUMBER=$release" >> $GITHUB_OUTPUT
              echo "DATE_NEW=$date_new" >> $GITHUB_OUTPUT
              
              # Compare dates and check if new release is available
              timestamp1=$(date -d "$date_new" +%s)
              timestamp2=$(date -d "$date" +%s)
              if [ "$timestamp1" -gt "$timestamp2" ]; then
                # Check if the file is accessible
                response=$(curl -o /dev/null -s -w "%{http_code}" "https://ftp.ebi.ac.uk/pub/databases/chebi/archive/rel${release}/SDF/ChEBI_complete_3star.sdf.gz")
                curl_exit_code=$?

                if [ $curl_exit_code -ne 0 ] || [ "$response" -ne 200 ]; then
                  echo "Error accessing ChEBI release (rel${release}): Exit code $curl_exit_code, Response $response"
                  echo "ISSUE=true" >> "$GITHUB_ENV"
                  echo "NEW_RELEASE=false" >> $GITHUB_OUTPUT
                else
                  echo "New ChEBI release available: $release"
                  echo "NEW_RELEASE=true" >> $GITHUB_OUTPUT
                fi
              else
                echo "No new ChEBI release available"
                echo "NEW_RELEASE=false" >> $GITHUB_OUTPUT
              fi
              # Clean up
              rm chebi_index.html
              ;;
              
            hgnc)
              echo 'Accessing the HGNC data using Puppeteer'
              # Create a Puppeteer script to extract the latest files
              cat <<EOF > download.js
              const puppeteer = require('puppeteer');

              (async () => {
                  const browser = await puppeteer.launch({
                      headless: true,
                      args: ['--no-sandbox', '--disable-setuid-sandbox']
                  });
                  const page = await browser.newPage();
                  await page.goto('https://www.genenames.org/download/archive/quarterly/tsv/', { waitUntil: 'networkidle0' });

                  // Get the page content
                  const content = await page.content();
                  const fs = require('fs');
                  fs.writeFileSync('hgnc_index.html', content);

                  await browser.close();
              })();
              EOF

              # Run the Puppeteer script
              node download.js

              # Extracting the latest complete and withdrawn files
              complete=$(grep -o 'hgnc_complete_set_[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\.txt' hgnc_index.html | tail -n 1)
              withdrawn=$(grep -o 'withdrawn_[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\.txt' hgnc_index.html | tail -n 1)
              date_new=$(echo "$complete" | awk -F '_' '{print $4}' | sed 's/\.txt//')
              
              echo "DATE_NEW=$date_new" >> $GITHUB_OUTPUT
              echo "COMPLETE_NEW=$complete" >> $GITHUB_OUTPUT
              echo "WITHDRAWN_NEW=$withdrawn" >> $GITHUB_OUTPUT
              
              # Check if files exist
              if [ -z "$complete" ] || [ -z "$withdrawn" ]; then
                echo "Could not find latest HGNC files"
                echo "ISSUE=true" >> "$GITHUB_ENV"
              fi
              
              # Clean up
              rm download.js hgnc_index.html
              ;;
              
            hmdb)
              echo 'Accessing the HMDB data'
              wget http://www.hmdb.ca/system/downloads/current/hmdb_metabolites.zip
              unzip hmdb_metabolites.zip
              
              # Extract date from XML
              date_new=$(head hmdb_metabolites.xml | grep 'update_date' | sed 's/.*>\([0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\).*/\1/')
              
              echo "DATE_NEW=$date_new" >> $GITHUB_OUTPUT
              
              # Compare dates
              timestamp1=$(date -d "$date_new" +%s)
              timestamp2=$(date -d "$date" +%s)
              if [ "$timestamp1" -gt "$timestamp2" ]; then
                echo "New HMDB release available"
                echo "NEW_RELEASE=true" >> $GITHUB_OUTPUT
              else
                echo "No new HMDB release available"
                echo "NEW_RELEASE=false" >> $GITHUB_OUTPUT
              fi
              
              # Clean up
              rm -f hmdb_metabolites.xml
              ;;
              
            ncbi)
              echo 'Accessing the NCBI data'
              last_modified=$(curl -sI https://ftp.ncbi.nih.gov/gene/DATA/gene_history.gz | grep -i Last-Modified)
              
              # Extract date from header
              date_new=$(echo $last_modified | cut -d':' -f2- | xargs -I {} date -d "{}" +%Y-%m-%d)
              
              echo "DATE_NEW=$date_new" >> $GITHUB_OUTPUT
              ;;
              
            uniprot)
              echo 'Accessing the UniProt data'
              wget https://ftp.ebi.ac.uk/pub/databases/uniprot/current_release/knowledgebase/complete/ -O uniprot_index.html
              
              # Extract date from latest release
              date_new=$(grep -oP 'uniprot_sprot\.fasta\.gz</a></td><td[^>]*>\K[0-9]{4}-[0-9]{2}-[0-9]{2}' uniprot_index.html)
              
              echo "DATE_NEW=$date_new" >> $GITHUB_OUTPUT
              
              # Clean up
              rm uniprot_index.html
              ;;
              
            wikidata)
              # Use current date as new date for Wikidata
              date_new=$(date +"%Y-%m-%d")
              echo "DATE_NEW=$date_new" >> $GITHUB_OUTPUT
              echo "NEW_RELEASE=true" >> $GITHUB_OUTPUT
              ;;
          esac
          
          echo "Date of latest release: $date_new"
          echo "Date of release of the current version: $date"

      - uses: JasonEtco/create-an-issue@v2
        if: ${{ env.ISSUE == 'true' }}
        name: Post issue about upstream data availability
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SOURCE: ${{ matrix.datasource }}
        with:
          filename: .github/ISSUE_TEMPLATE/ISSUE_DATA.md
          update_existing: true

  process_data:
    needs: check_new_release
    if: |
      needs.check_new_release.outputs.NEW_RELEASE == 'true' ||
      !contains(fromJSON('["chebi", "hmdb"]'), needs.check_new_release.outputs.DATASOURCE)
    name: Process ${{ matrix.datasource }} data
    strategy:
      fail-fast: false
      matrix:
        datasource: ${{ fromJson(needs.setup.outputs.matrix).datasource }}
    env:
      DATASOURCE: ${{ matrix.datasource }}
      RELEASE_NUMBER: ${{ needs.check_new_release.outputs.RELEASE_NUMBER }}
      DATE_OLD: ${{ needs.check_new_release.outputs.DATE_OLD }}
      DATE_NEW: ${{ needs.check_new_release.outputs.DATE_NEW }}
      CURRENT_RELEASE_NUMBER: ${{ needs.check_new_release.outputs.CURRENT_RELEASE_NUMBER }}
      COMPLETE_NEW: ${{ needs.check_new_release.outputs.COMPLETE_NEW }}
      WITHDRAWN_NEW: ${{ needs.check_new_release.outputs.WITHDRAWN_NEW }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # Set up Java for ChEBI and HMDB
      - name: Set up Java
        if: matrix.datasource == 'chebi' || matrix.datasource == 'hmdb'
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      # Set up R for HGNC, NCBI, and UniProt
      - name: Install R
        if: matrix.datasource == 'hgnc' || matrix.datasource == 'ncbi' || matrix.datasource == 'uniprot'
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.1'
          include-recommended: true

      # Install XML tools for HMDB
      - name: Install XML tools
        if: matrix.datasource == 'hmdb'
        run: sudo apt-get install -y xml-twig-tools

      - name: Download and process ${{ matrix.datasource }} data
        id: process_data
        run: |
          # Create the output directory
          mkdir -p datasources/${DATASOURCE}/recentData
          
          case "${DATASOURCE}" in
            chebi)
              # Download ChEBI SDF file
              wget "https://ftp.ebi.ac.uk/pub/databases/chebi/archive/rel${RELEASE_NUMBER}/SDF/ChEBI_complete_3star.sdf.gz"
              gunzip ChEBI_complete_3star.sdf.gz
              
              inputFile="ChEBI_complete_3star.sdf"
              outputDir="datasources/chebi/recentData/"
              
              # Process the data
              java -cp java/target/mapping_prerocessing-0.0.1-jar-with-dependencies.jar org.sec2pri.chebi_sdf "$inputFile" "$outputDir"
              
              if [ $? -eq 0 ]; then
                echo "Successful preprocessing of ChEBI data."
                echo "FAILED=false" >> $GITHUB_ENV
              else
                echo "Failed preprocessing of ChEBI data."
                echo "FAILED=true" >> $GITHUB_ENV
              fi
              ;;
              
            hgnc)
              # Download HGNC files
              wget https://storage.googleapis.com/public-download-files/hgnc/archive/archive/quarterly/tsv/${WITHDRAWN_NEW}
              wget https://storage.googleapis.com/public-download-files/hgnc/archive/archive/quarterly/tsv/${COMPLETE_NEW}
              mkdir -p datasources/hgnc/data
              mv $WITHDRAWN_NEW $COMPLETE_NEW datasources/hgnc/data
              
              # Process the data
              sourceVersion=$DATE_NEW
              complete="datasources/hgnc/data/${COMPLETE_NEW}"
              withdrawn="datasources/hgnc/data/${WITHDRAWN_NEW}"
              
              Rscript r/src/hgnc.R $sourceVersion $withdrawn $complete
              
              if [ $? -eq 0 ]; then
                echo "Successful preprocessing of HGNC data."
                echo "FAILED=false" >> $GITHUB_ENV
              else
                echo "Failed preprocessing of HGNC data."
                echo "FAILED=true" >> $GITHUB_ENV
              fi
              ;;
              
            hmdb)
              # Download HMDB files
              wget http://www.hmdb.ca/system/downloads/current/hmdb_metabolites.zip
              unzip hmdb_metabolites.zip
              mkdir hmdb
              mv hmdb_metabolites.xml hmdb
              cd hmdb
              xml_split -v -l 1 hmdb_metabolites.xml
              rm hmdb_metabolites.xml
              cd ../
              zip -r hmdb_metabolites_split.zip hmdb
              
              # Process the data
              inputFile=hmdb_metabolites_split.zip
              outputDir="datasources/hmdb/recentData/"
              
              java -cp java/target/mapping_prerocessing-0.0.1-jar-with-dependencies.jar org.sec2pri.hmdb_xml "$inputFile" "$outputDir"
              
              if [ $? -eq 0 ]; then
                echo "Successful preprocessing of HMDB data."
                echo "FAILED=false" >> $GITHUB_ENV
              else
                echo "Failed preprocessing of HMDB data."
                echo "FAILED=true" >> $GITHUB_ENV
              fi
              ;;
              
            ncbi)
              # Download NCBI files
              mkdir -p datasources/ncbi/data
              wget https://ftp.ncbi.nih.gov/gene/DATA/gene_info.gz
              wget https://ftp.ncbi.nih.gov/gene/DATA/gene_history.gz
              mv gene_info.gz gene_history.gz datasources/ncbi/data
              
              # Process the data
              sourceVersion=$DATE_NEW
              gene_history="data/gene_history.gz"
              gene_info="data/gene_info.gz"
              
              Rscript r/src/ncbi.R $sourceVersion $gene_history $gene_info
              
              if [ $? -eq 0 ]; then
                echo "Successful preprocessing of NCBI data."
                echo "FAILED=false" >> $GITHUB_ENV
              else
                echo "Failed preprocessing of NCBI data."
                echo "FAILED=true" >> $GITHUB_ENV
              fi
              ;;
              
            uniprot)
              # Download UniProt files
              mkdir -p datasources/uniprot/data
              wget https://ftp.ebi.ac.uk/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz
              wget https://ftp.ebi.ac.uk/pub/databases/uniprot/current_release/knowledgebase/complete/docs/sec_ac.txt
              wget https://ftp.ebi.ac.uk/pub/databases/uniprot/current_release/knowledgebase/complete/docs/delac_sp.txt
              mv delac_sp.txt sec_ac.txt uniprot_sprot.fasta.gz datasources/uniprot/data
              
              # Process the data
              sourceVersion=$DATE_NEW
              uniprot_sprot="datasources/uniprot/data/uniprot_sprot.fasta.gz"
              sec_ac="datasources/uniprot/data/sec_ac.txt"
              delac_sp="datasources/uniprot/data/delac_sp.txt"
              
              Rscript r/src/uniprot.R $sourceVersion $uniprot_sprot $delac_sp $sec_ac
              
              if [ $? -eq 0 ]; then
                echo "Successful preprocessing of UniProt data."
                echo "FAILED=false" >> $GITHUB_ENV
              else
                echo "Failed preprocessing of UniProt data."
                echo "FAILED=true" >> $GITHUB_ENV
              fi
              ;;
              
            wikidata)
              # Create output directory
              mkdir -p datasources/wikidata/recentData/
              
              # Download Wikidata query results
              curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/chemicalAllRedirects.rq -G https://qlever.cs.uni-freiburg.de/api/wikidata -o datasources/wikidata/recentData/metabolites_secID2priID_qlever.tsv
              curl -H "Accept: text/tab-separated-values" --data-urlencode query@datasources/wikidata/queries/geneproteinHumanAllRedirects.rq -G https://qlever.cs.uni-freiburg.de/api/wikidata -o datasources/wikidata/recentData/geneProtein_secID2priID_qlever.tsv
              
              # Check for query timeouts
              cd datasources/wikidata/recentData
              fail_file=''
              for File in *.tsv
              do
                if grep -q TimeoutException "$File"; then
                  echo "Query Timeout occurred for file: $File"
                  fail_file="${fail_file} $File"
                  echo "FAILED=true" >> $GITHUB_ENV
                else
                  echo "No Query Timeout detected for file: $File"
                fi
              done
              
              # Process Wikidata files - remove IRIs and format properly
              prefix="Wikidata"
              for f in *.tsv
              do
                cat "$f" | sed 's/<http:\/\/www.wikidata.org\/entity\///g' | sed 's/[>]//g' | sed 's/@en//g' > "${prefix}_$f"
                rm "$f"
              done
              cd ../../..
              
              if [ -z "$fail_file" ]; then
                echo "FAILED=false" >> $GITHUB_ENV
              fi
              ;;
          esac

      - name: Validate and compare data
        if: env.FAILED == 'false' && matrix.datasource != 'wikidata'
        run: |
          to_check_from_zenodo=$(grep -E '^to_check_from_zenodo=' datasources/${DATASOURCE}/config | cut -d'=' -f2)
          old="datasources/${DATASOURCE}/data/$to_check_from_zenodo"
          new="datasources/${DATASOURCE}/recentData/$to_check_from_zenodo"
          
          # Handle special cases for different datasources
          case "${DATASOURCE}" in
            ncbi|uniprot)
              # Unzip comparison file for NCBI and UniProt
              unzip datasources/${DATASOURCE}/data/${DATASOURCE}_secID2priID.zip -d datasources/${DATASOURCE}/data/
              ;;
          esac
          
          # Remove headers from files
          if [ "${DATASOURCE}" != "hgnc" ]; then
            sed -i '1d' "$new"
            sed -i '1d' "$old"
          fi
          
          # Validate ID formats with RegEx
          if [ "${DATASOURCE}" != "hgnc" ] && [ "${DATASOURCE}" != "wikidata" ]; then
            wget -nc https://raw.githubusercontent.com/bridgedb/datasources/main/datasources.tsv
            
            # Map datasource name to pattern name in datasources.tsv
            pattern_name="${DATASOURCE}"
            if [ "${DATASOURCE}" == "ncbi" ]; then
              pattern_name="Entrez Gene"
            fi
            
            DS_ID=$(awk -F '\t' '$1 == "'$pattern_name'" {print $10}' datasources.tsv)
            
            # Split columns for validation
            awk -F '\t' '{print $1}' $new > column1.txt
            awk -F '\t' '{print $2}' $new > column2.txt
            
            # Validate primary IDs
            if grep -nqvE "$DS_ID" "column1.txt"; then
              echo "All lines in primary column match pattern."
            else
              echo "Error: Lines in primary column don't match pattern."
              grep -nvE "^$DS_ID$" "column1.txt"
              echo "FAILED=true" >> $GITHUB_ENV
              exit 1
            fi
            
            # Validate secondary IDs
            if grep -nqvE "$DS_ID" "column2.txt"; then
              echo "All lines in secondary column match pattern."
            else
              echo "Error: Lines in secondary column don't match pattern."
              grep -nqvE "$DS_ID" "column2.txt"
              echo "FAILED=true" >> $GITHUB_ENV
              exit 1
            fi
          fi
          
          # Process files based on datasource
          case "${DATASOURCE}" in
            hgnc|ncbi)
              # Extract ID columns from HGNC and NCBI
              cat "$old" | sort | tr -d "\r" | cut -f 1,3 > ids_old.txt
              cat "$new" | sort | tr -d "\r" | cut -f 1,3 > ids_new.txt
              ;;
            uniprot)
              # Extract ID columns from UniProt
              cat "$old" | sort | tr -d "\r" | cut -f 1,2 > ids_old.txt
              cat "$new" | sort | tr -d "\r" | cut -f 1,2 > ids_new.txt
              ;;
            *)
              # For other datasources, sort the entire file
              cat "$old" | sort | tr -d "\r" > ids_old.txt
              cat "$new" | sort | tr -d "\r" > ids_new.txt
              ;;
          esac
          
          # Compare the files
          output_file=diff.txt
          diff -u ids_old.txt ids_new.txt > $output_file || true
          
          # Find added and removed lines
          added=$(grep '^+' "$output_file" | sed 's/^+//g') || true
          removed=$(grep '^-' "$output_file" | sed 's/^-//g') || true
          
          # Filter out lines that appear in both added and removed (changes)
          added_filtered=$(comm -23 <(sort <<< "$added") <(sort <<< "$removed"))
          removed_filtered=$(comm -23 <(sort <<< "$removed") <(sort <<< "$added"))
          added=$added_filtered
          removed=$removed_filtered
          
          # Count changes
          count_removed=$(printf "$removed" | wc -l) || true
          count_added=$(printf "$added" | wc -l) || true
          
          # Handle empty results
          if [ -z "$removed" ]; then
            count_removed=0
            removed="None"
          fi
          if [ -z "$added" ]; then
            count_added=0
            added="None"
          fi
          
          # Display results
          echo "================================================"
          echo "                Removed pairs                   "
          echo "================================================"
          echo "$removed"
          echo "================================================"
          echo "                 Added pairs                    "
          echo "================================================"
          echo "$added"
          echo "================================================"
          echo "What's changed:"
          echo "- Added id pairs: $count_added"
          echo "- Removed id pairs: $count_removed"
          
          # Store metrics in environment variables
          echo "ADDED=$count_added" >> $GITHUB_ENV
          echo "REMOVED=$count_removed" >> $GITHUB_ENV
          count=$(expr $count_added + $count_removed) || true
          echo "COUNT=$count" >> $GITHUB_ENV
          
          # Calculate percentage of change
          total_old=$(cat "$old" | wc -l) || true 
          change=$((100 * count / total_old))
          echo "CHANGE=$change" >> $GITHUB_ENV

      - name: 'Upload processed data as artifacts'
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.datasource }}_processed
          path: datasources/${{ matrix.datasource }}/recentData/*

      - uses: JasonEtco/create-an-issue@v2
        if: env.COUNT != 0 && env.FAILED == 'false' && matrix.datasource != 'wikidata'
        name: Post issue about update availability
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SOURCE: ${{ matrix.datasource }}
        with:
          filename: .github/ISSUE_TEMPLATE/ISSUE_UPDATE.md
          update_existing: true

      - uses: JasonEtco/create-an-issue@v2
        if: env.FAILED == 'true'
        name: Post issue about failing test
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SOURCE: ${{ matrix.datasource }}
        with:
          filename: .github/ISSUE_TEMPLATE/ISSUE_FAIL.md
          update_existing: true